# How to Run APERTUS - The Swiss LLM Model - on Azure Container Apps

> [!NOTE]
> **COMING SOON**

Serverless GPUs accelerate AI development by allowing you to focus on your core AI code and less on managing infrastructure when using GPUs. This feature provides a middle layer option between the Azure AI model catalog's serverless APIs and hosting models on managed compute.

The Container Apps serverless GPU support provides full data governance as your data never leaves the boundaries of your container while still providing a managed, serverless platform from which to build your applications.

When you use serverless GPUs in Container Apps, your apps get:

Scale-to zero GPUs: Support for automatic serverless scaling of NVIDIA A100 and NVIDIA T4 GPUs.

Per-second billing: Pay only for the GPU compute you use.

Built-in data governance: Your data never leaves the container boundary.

Flexible compute options: You can choose between the NVIDIA A100 or T4 GPU types.

Middle-layer for AI development: Bring your own model on a managed, serverless compute platform.